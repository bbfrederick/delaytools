#!/usr/bin/env python
# -*- coding: latin-1 -*-
#
#   Copyright 2016-2021 Blaise Frederick
#
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
#
#
from __future__ import print_function, division
import sys
import pandas as pd
import rapidtide.miscmath as tide_math
import rapidtide.util as tide_util
import rapidtide.io as tide_io
import rapidtide.fit as tide_fit
import rapidtide.filter as tide_filt
import rapidtide.correlate as tide_corr
from scipy.stats.stats import pearsonr
import numpy as np
import nibabel as nib
import argparse
import rapidtide.workflows.parser_funcs as pf

import matplotlib.pyplot as plt


def getNullDistributionData(
    indata,
    xcorr_x,
    thefilter,
    windowfunc,
    detrendorder,
    searchstart,
    searchend,
    Fs,
    corrweighting="None",
    numreps=1000,
):
    print("estimating significance distribution using ", numreps, " repetitions")
    corrlist = np.zeros((numreps), dtype="float")
    corrlist_pear = np.zeros((numreps), dtype="float")
    xcorr_x_trim = xcorr_x[searchstart : searchend + 1]

    filteredindata = tide_math.corrnormalize(
        thefilter.apply(Fs, indata), windowfunc=windowfunc, detrendorder=detrendorder
    )
    for i in range(0, numreps):
        # make a shuffled copy of the regressors
        shuffleddata = np.random.permutation(indata)

        # filter it
        filteredshuffleddata = tide_math.corrnormalize(
            thefilter.apply(Fs, shuffleddata),
            windowfunc=windowfunc,
            detrendorder=detrendorder,
        )

        # crosscorrelate with original
        theshuffledxcorr = tide_corr.fastcorrelate(
            filteredindata,
            filteredshuffleddata,
            usefft=True,
            weighting=corrweighting,
        )

        # find and tabulate correlation coefficient at optimal lag
        theshuffledxcorr_trim = theshuffledxcorr[searchstart : searchend + 1]
        maxdelay = xcorr_x_trim[np.argmax(theshuffledxcorr_trim)]
        corrlist[i] = theshuffledxcorr_trim[np.argmax(theshuffledxcorr_trim)]

        # find and tabulate correlation coefficient at 0 lag
        corrlist_pear[i] = pearsonr(filteredindata, filteredshuffleddata)[0]

        # progress
        tide_util.progressbar(i + 1, numreps, label="Percent complete")

    # jump to line after progress bar
    print()

    # return the distribution data
    return corrlist, corrlist_pear


def printthresholds(pcts, thepercentiles, labeltext):
    print(labeltext)
    for i in range(0, len(pcts)):
        print("\tp <", 1.0 - thepercentiles[i], ": ", pcts[i])


def _get_parser():
    parser = argparse.ArgumentParser(
        prog="showstxcorr",
        description="Plots the data in text files.",
        usage="%(prog)s infilename1 infilename2 outfileroot [options]",
    )

    pf.addreqinputtextfile(parser, "infilename1", onecol=True)
    pf.addreqinputtextfile(parser, "infilename2", onecol=True)
    parser.add_argument(
        "outfilename",
        type=str,
        action="store",
        help="The root name of the output files.",
    )

    sampling = parser.add_mutually_exclusive_group()
    sampling.add_argument(
        "--samplerate",
        dest="samplerate",
        action="store",
        metavar="FREQ",
        type=lambda x: pf.is_float(parser, x),
        help=(
            "Set the sample rate of the data file to FREQ. "
            "If neither samplerate or sampletime is specified, sample rate is 1.0."
        ),
        default="auto",
    )
    sampling.add_argument(
        "--sampletime",
        dest="samplerate",
        action="store",
        metavar="TSTEP",
        type=lambda x: pf.invert_float(parser, x),
        help=(
            "Set the sample rate of the data file to 1.0/TSTEP. "
            "If neither samplerate or sampletime is specified, sample rate is 1.0."
        ),
        default="auto",
    )
    # add optional arguments
    parser.add_argument(
        "--windowwidth",
        dest="windowwidth",
        metavar="WINDOWWIDTH",
        type=lambda x: pf.is_float(parser, x),
        help="Use a window width of WINDOWWIDTH seconds (default is 50.0s).",
        default=50.0,
    )
    parser.add_argument(
        "--stepsize",
        dest="stepsize",
        metavar="STEPSIZE",
        type=lambda x: pf.is_float(parser, x),
        help=(
            "Timestep between subsequent measurements (default is 25.0s).  "
            "Will be rounded to the nearest sample time."
        ),
        default=25.0,
    )
    parser.add_argument(
        "--starttime",
        dest="starttime",
        metavar="START",
        type=float,
        help="Start plotting at START seconds (default is 0.0).",
        default=0.0,
    )
    parser.add_argument(
        "--duration",
        dest="duration",
        metavar="DURATION",
        type=float,
        help="Amount of data, in seconds, to process after starttime (default is the entire timecourse).",
        default=1000000.0,
    )
    parser.add_argument(
        "--nodisplay",
        dest="display",
        action="store_false",
        help=("Do not plot the data (for noninteractive use)"),
        default=True,
    )
    parser.add_argument(
        "--debug",
        dest="debug",
        action="store_true",
        help=("Enable additional debugging output."),
        default=False,
    )
    parser.add_argument(
        "--verbose",
        dest="verbose",
        action="store_true",
        help=("Print out more debugging information"),
        default=False,
    )
    pf.addsearchrangeopts(parser, details=True)
    pf.addtimerangeopts(parser)
    parser.add_argument(
        "--trimdata",
        dest="trimdata",
        action="store_true",
        help=("Trimming data to match"),
        default=False,
    )

    preproc = parser.add_argument_group()
    preproc.add_argument(
        "--detrendorder",
        dest="detrendorder",
        action="store",
        type=int,
        metavar="ORDER",
        help=("Set order of trend removal (0 to disable, default is 1 - linear). "),
        default=1,
    )
    # add window options
    pf.addwindowopts(parser)

    # Filter arguments
    pf.addfilteropts(parser, "timecourses", details=True)

    # Preprocessing options
    preproc = parser.add_argument_group("Preprocessing options")
    preproc.add_argument(
        "--corrweighting",
        dest="corrweighting",
        action="store",
        type=str,
        choices=["None", "phat", "liang", "eckart"],
        help=("Method to use for cross-correlation " "weighting. Default is  None. "),
        default="None",
    )
    preproc.add_argument(
        "--invert",
        dest="invert",
        action="store_true",
        help=("Invert second timecourse prior to correlation. "),
        default=False,
    )
    preproc.add_argument(
        "--label",
        dest="label",
        metavar="LABEL",
        action="store",
        type=str,
        help=("Label for the delay value. "),
        default="None",
    )
    return parser


def usage():
    print("")
    print(
        "showstxcorr - calculate and display the short term crosscorrelation between two timeseries"
    )
    print("")
    print(
        "usage: showstxcorr -i timecoursefile1 [-i timecoursefile2] --samplefreq=FREQ -o outputfile [-l LABEL] [-s STARTTIME] [-D DURATION] [-d] [-F LOWERFREQ,UPPERFREQ[,LOWERSTOP,UPPERSTOP]] [-V] [-L] [-R] [-C] [--nodetrend] [-nowindow] [-f] [--phat] [--liang] [--eckart] [-z FILENAME]"
    )
    print("optional arguments:")
    print("    --nodetrend   - do not detrend the data before correlation")
    print("    --nowindow    - do not window data before corrlation")
    print("    --phat        - perform phase alignment transform (PHAT) rather than ")
    print("                    standard crosscorrelation")
    print(
        "    --liang       - perform phase alignment transform with Liang weighting function rather than "
    )
    print("                    standard crosscorrelation")
    print(
        "    --eckart      - perform phase alignment transform with Eckart weighting function rather than "
    )
    print("                    standard crosscorrelation")
    # print("    -l LABEL      - label for the delay value")
    print("    -s STARTTIME  - time of first datapoint to use in seconds in the first file")
    print("    -D DURATION   - amount of data to use in seconds")
    print("    -d            - turns off display of graph")
    print("    -F            - filter data and regressors from LOWERFREQ to UPPERFREQ.")
    print(
        "                    LOWERSTOP and UPPERSTOP can be specified, or will be calculated automatically"
    )
    print("    -V            - filter data and regressors to VLF band")
    print("    -L            - filter data and regressors to LFO band")
    print("    -R            - filter data and regressors to respiratory band")
    print("    -C            - filter data and regressors to cardiac band")
    # print("    -A            - print data on a single summary line")
    # print("    -a            - if summary mode is on, add a header line showing what values mean")
    # print("    -z FILENAME   - use the columns of FILENAME as controlling variables and return the partial correlation")
    print("    -W WINDOWLEN  - use a window length of WINDOWLEN seconds (default is 50.0s)")
    print(
        "    -S STEPSIZE   - timestep between subsequent measurements (default is 25.0s).  Will be rounded to the nearest sample time"
    )
    # print("    -N TRIALS     - estimate significance thresholds by Monte Carlo with TRIALS repetition")
    print("    -f            - negate second regressor")
    # print("    -i            - output intermediate steps in crosscorrelation calculation")
    print("")
    return ()


def main():
    # get the command line parameters
    verbose = True
    windowtime = 50.0
    window_lowestfreq = 1.0 / windowtime
    window_upperfreq = 0.15
    stepsize = 25.0
    matrixoutput = False

    """# scan the command line arguments
    try:
        opts, args = getopt.gnu_getopt(
            sys.argv[1:],
            "i:IfN:W:S:z:aATtVLRCF:dl:s:D:wgo:",
            [
                "infile=",
                "outfile=",
                "phat",
                "liang",
                "eckart",
                "nodetrend",
                "nowindow",
                "samplefreq=",
                "sampletime=",
                "help",
            ],
        )
    except getopt.GetoptError as err:
        # print help information and exit:
        print(str(err))  # will print something like "option -x not recognized"
        usage()
        sys.exit(2)

    if len(args) > 1:
        print("showstxcorr takes no unflagged arguments")
        usage()
        exit()"""

    # grab the command line arguments then pass them off.
    try:
        args = _get_parser().parse_args()
    except SystemExit:
        _get_parser().print_help()
        raise

    # finish up processing arguments, do initial filter setup
    args, theprefilter = pf.postprocessfilteropts(args)
    args = pf.postprocesssearchrangeopts(args)
    args = pf.postprocesstimerangeopts(args)

    """theprefilter = tide_filt.noncausalfilter()

    # set the default characteristics
    print("setting default filter to pass", window_lowestfreq, "to", window_upperfreq)
    theprefilter.settype("arb")
    theprefilter.setfreqs(
        window_lowestfreq,
        window_lowestfreq * 1.05,
        window_upperfreq / 1.05,
        window_upperfreq,
    )

    for o, a in opts:
        if o == "-d":
            display = False
            if verbose:
                print("disable display")
        elif o == "--infile" or o == "-i":
            infilename.append(a)
            if verbose:
                print("will use", infilename[-1], "as an input file")
        elif o == "--outfile" or o == "-o":
            outfilename = a
            if verbose:
                print("will use", outfilename, "as output file")
        elif o == "-T":
            trimdata = True
            if verbose:
                print("trimming data to match")
        elif o == "--samplefreq":
            Fs = float(a)
            sampletime = 1.0 / Fs
            linkchar = "="
            if verbose:
                print("Setting sample frequency to ", Fs)
        elif o == "--sampletime":
            sampletime = float(a)
            Fs = 1.0 / sampletime
            linkchar = "="
            if verbose:
                print("Setting sample time step to ", sampletime)
        elif o == "--liang":
            corrweighting = "liang"
            if verbose:
                print("doing Liang weighted correlation")
        elif o == "--eckart":
            corrweighting = "eckart"
            if verbose:
                print("doing Eckart weighted correlation")
        elif o == "--phat":
            corrweighting = "phat"
            if verbose:
                print("doing phase alignment transform")
        elif o == "-I":
            dumpfiltered = True
            if verbose:
                print("outputting intermediate calculations")
        elif o == "-f":
            flipregressor = True
            if verbose:
                print("negating second regressor")
        elif o == "-a":
            labelline = True
            if verbose:
                print("turning on label line")
        elif o == "-t":
            print(
                "DEPRECATION WARNING: detrending is now on by default.  Use --nodetrend to disable it"
            )
        elif o == "--nodetrend":
            detrendorder = 0
            if verbose:
                print("disabling detrending")
        elif o == "-w":
            print(
                "DEPRECATION WARNING: windowing is now on by default.  Use --nowindow to disable it"
            )
        elif o == "--nowindow":
            windowfunc = "None"
            if verbose:
                print("disabling windowing")
        elif o == "-z":
            controlvariablefile = a
            dopartial = True
            if verbose:
                print("performing partial correlations")
        elif o == "-l":
            thelabel = a
            uselabel = True
            if verbose:
                print("label set to", thelabel)
        elif o == "-D":
            duration = float(a)
            if verbose:
                print("duration set to", duration)
        elif o == "-W":
            windowtime = float(a)
            window_lowestfreq = 1.0 / windowtime
            theprefilter.settype("arb")
            theprefilter.setfreqs(
                window_lowestfreq,
                window_lowestfreq * 1.05,
                window_upperfreq / 1.05,
                window_upperfreq,
            )
            if verbose:
                print("windowtime set to", windowtime)
                print(
                    "setting default filter to pass",
                    window_lowestfreq,
                    "to",
                    window_upperfreq,
                )
        elif o == "-S":
            stepsize = sampletime * np.round(float(a) / sampletime)
            if verbose:
                print("stepsize set to", stepsize)
        elif o == "-N":
            numreps = int(a)
            estimate_significance = True
            if verbose:
                print("estimating significance threshold with ", numreps, " trials")
        elif o == "-s":
            starttime = float(a)
            if verbose:
                print("starttime set to", starttime)
        elif o == "-V":
            theprefilter.settype("vlf")
            if verbose:
                print("prefiltering to vlf band")
        elif o == "-L":
            theprefilter.settype("lfo")
            if verbose:
                print("prefiltering to lfo band")
        elif o == "-R":
            theprefilter.settype("resp")
            if verbose:
                print("prefiltering to respiratory band")
        elif o == "-C":
            theprefilter.settype("cardiac")
            if verbose:
                print("prefiltering to cardiac band")
        elif o == "-A":
            verbose = False
            summarymode = True
        elif o == "-F":
            arbvec = a.split(",")
            if len(arbvec) != 2 and len(arbvec) != 4:
                usage()
                sys.exit()
            if len(arbvec) == 2:
                arb_lower = float(arbvec[0])
                arb_upper = float(arbvec[1])
                arb_lowerstop = 0.9 * float(arbvec[0])
                arb_upperstop = 1.1 * float(arbvec[1])
            if len(arbvec) == 4:
                arb_lower = float(arbvec[0])
                arb_upper = float(arbvec[1])
                arb_lowerstop = float(arbvec[2])
                arb_upperstop = float(arbvec[3])
            theprefilter.settype("arb")
            theprefilter.setfreqs(arb_lowerstop, arb_lower, arb_upper, arb_upperstop)
            if verbose:
                print(
                    "prefiltering to ",
                    arb_lower,
                    arb_upper,
                    "(stops at ",
                    arb_lowerstop,
                    arb_upperstop,
                    ")",
                )
        else:
            #assert False, "unhandled option"
            pass"""

    # check that required arguments are set
    if args.samplerate == "auto":
        print("samplerate must be set")
        sys.exit()
    sampletime = 1.0 / args.samplerate

    # read in the files and get everything trimmed to the right length
    stepsize = sampletime * np.round(args.stepsize / sampletime)

    # Now update the lower limit of the filter.
    init_lowerstop, init_lowerpass, init_upperpass, init_upperstop = theprefilter.getfreqs()
    window_lowestfreq = 1.0 / args.windowwidth
    if window_lowestfreq > init_lowerpass:
        print(
            f"resetting lower limit of filter from {init_lowerpass} to {window_lowestfreq}Hz for window length {args.windowwidth}s"
        )
        theprefilter.settype("arb")
        theprefilter.setfreqs(
            window_lowestfreq,
            window_lowestfreq,
            init_upperpass,
            init_upperstop,
        )

    startpoint = max([int(args.starttime * args.samplerate), 0])
    inputdata1 = tide_io.readvec(args.infilename1)
    numpoints = len(inputdata1)
    inputdata2 = tide_io.readvec(args.infilename2)
    endpoint1 = min(
        [
            startpoint + int(args.duration * args.samplerate),
            int(len(inputdata1)),
            int(len(inputdata2)),
        ]
    )
    endpoint2 = min(
        [int(args.duration * args.samplerate), int(len(inputdata1)), int(len(inputdata2))]
    )
    trimmeddata = np.zeros((2, numpoints), dtype="float")
    trimmeddata[0, :] = inputdata1[startpoint:endpoint1]
    trimmeddata[1, :] = inputdata2[0:endpoint2]

    # band limit the regressors if that is needed
    if theprefilter.gettype() != "None":
        if verbose:
            print("filtering to ", theprefilter.gettype(), " band")

    thedims = trimmeddata.shape
    tclen = thedims[1]
    numcomponents = thedims[0]
    reformdata = np.reshape(trimmeddata, (numcomponents, tclen))

    print("preprocessing all timecourses")
    for component in range(0, numcomponents):
        filtereddata = tide_math.corrnormalize(
            theprefilter.apply(args.samplerate, reformdata[component, :]),
            windowfunc="None",
            detrendorder=args.detrendorder,
        )
        reformdata[component, :] = tide_math.stdnormalize(
            tide_fit.detrend(tide_math.stdnormalize(filtereddata), order=args.detrendorder)
        )

    xcorr_x = np.r_[0.0:tclen] * sampletime - (tclen * sampletime) / 2.0
    laglimit = 15.0
    widthlimit = 15.0
    halfwindow = int(laglimit * args.samplerate)
    searchstart = int(int(tclen) // 2 - halfwindow)
    searchend = int(int(tclen) // 2 + halfwindow)
    xcorr_x_trim = xcorr_x[searchstart:searchend]
    if args.invert:
        flipfac = -1.0
    else:
        flipfac = 1.0

    # now that we have all the information, we have a couple of places to go:
    # We are either doing short term correlations or full timecourse
    # We are either doing two time courses from different (or the same) files, or we are doing more than 2

    if matrixoutput:
        # find the lengths of the outputfiles
        print("finding timecourse lengths")
        times, corrpertime, ppertime = tide_corr.shorttermcorr_1D(
            reformdata[0, :],
            reformdata[0, :],
            sampletime,
            windowtime,
            samplestep=int(stepsize // sampletime),
            windowfunc=args.windowfunc,
            detrendorder=0,
        )
        plength = len(times)
        times, xcorrpertime, Rvals, delayvals, valid = tide_corr.shorttermcorr_2D(
            reformdata[0, :],
            reformdata[0, :],
            sampletime,
            windowtime,
            samplestep=int(stepsize // sampletime),
            weighting=args.corrweighting,
            windowfunc=args.windowfunc,
            detrendorder=0,
            display=False,
        )
        xlength = len(times)

        # now allocate the output arrays
        print("allocating data arrays")
        Rvals = np.zeros((numcomponents, numcomponents, 1, xlength), dtype="float")
        delayvals = np.zeros((numcomponents, numcomponents, 1, xlength), dtype="float")
        valid = np.zeros((numcomponents, numcomponents, 1, xlength), dtype="float")
        corrpertime = np.zeros((numcomponents, numcomponents, 1, plength), dtype="float")
        ppertime = np.zeros((numcomponents, numcomponents, 1, plength), dtype="float")

        # do the correlations
        for component1 in range(0, numcomponents):
            print("correlating with component", component1)
            for component2 in range(0, numcomponents):
                (
                    times,
                    corrpertime[component1, component2, 0, :],
                    ppertime[component1, component2, 0, :],
                ) = tide_corr.shorttermcorr_1D(
                    reformdata[component1, :],
                    flipfac * reformdata[component2, :],
                    sampletime,
                    windowtime,
                    samplestep=int(stepsize // sampletime),
                    windowfunc=args.windowfunc,
                    detrendorder=0,
                )
                (
                    times,
                    xcorrpertime,
                    Rvals[component1, component2, 0, :],
                    delayvals[component1, component2, 0, :],
                    valid[component1, component2, 0, :],
                ) = tide_corr.shorttermcorr_2D(
                    reformdata[component1, :],
                    flipfac * reformdata[component2, :],
                    sampletime,
                    windowtime,
                    samplestep=int(stepsize // sampletime),
                    weighting=args.corrweighting,
                    windowfunc=args.windowfunc,
                    detrendorder=0,
                    display=False,
                )

        outputaffine = np.eye(4)
        # input_img, input_data, input_hdr, thedims, thesizes = tide_io.readfromnifti(inputfilename)
        init_img = nib.Nifti1Image(corrpertime, outputaffine)
        init_hdr = init_img.header.copy()
        init_sizes = init_hdr["pixdim"].copy()
        init_sizes[4] = sampletime
        init_hdr["toffset"] = times[0]
        tide_io.savetonifti(corrpertime, init_hdr, args.outfilename + "_pearsonR")
        tide_io.savetonifti(ppertime, init_hdr, args.outfilename + "_corrp")
        tide_io.savetonifti(Rvals, init_hdr, args.outfilename + "_maxxcorr")
        tide_io.savetonifti(delayvals, init_hdr, args.outfilename + "_delayvals")
        tide_io.savetonifti(valid, init_hdr, args.outfilename + "_valid")
        rows = []
        cols = []
        for i in range(numcomponents):
            rows.append("region " + str(i + 1))
            cols.append("region " + str(i + 1))
        for segment in range(plength):
            df = pd.DataFrame(data=corrpertime[:, :, 0, 0], columns=cols)
            df.insert(0, "sources", pd.Series(rows))
            df.to_csv(
                args.outfilename + "_seg_" + str(segment).zfill(4) + "_pearsonR.csv",
                index=False,
            )
            df = pd.DataFrame(data=ppertime[:, :, 0, 0], columns=cols)
            df.insert(0, "sources", pd.Series(rows))
            df.to_csv(
                args.outfilename + "_seg_" + str(segment).zfill(4) + "_corrp.csv",
                index=False,
            )
        for segment in range(xlength):
            df = pd.DataFrame(data=Rvals[:, :, 0, 0], columns=cols)
            df.insert(0, "sources", pd.Series(rows))
            df.to_csv(
                args.outfilename + "_seg_" + str(segment).zfill(4) + "_maxxcorr.csv",
                index=False,
            )
            df = pd.DataFrame(data=delayvals[:, :, 0, 0], columns=cols)
            df.insert(0, "sources", pd.Series(rows))
            df.to_csv(
                args.outfilename + "_seg_" + str(segment).zfill(4) + "_delayvals.csv",
                index=False,
            )
            df = pd.DataFrame(data=valid[:, :, 0, 0], columns=cols)
            df.insert(0, "sources", pd.Series(rows))
            df.to_csv(
                args.outfilename + "_seg_" + str(segment).zfill(4) + "_valid.csv",
                index=False,
            )

    else:
        times, corrpertime, ppertime = tide_corr.shorttermcorr_1D(
            reformdata[0, :],
            flipfac * reformdata[1, :],
            sampletime,
            windowtime,
            samplestep=int(stepsize // sampletime),
            windowfunc=args.windowfunc,
            detrendorder=0,
        )
        times, xcorrpertime, Rvals, delayvals, valid = tide_corr.shorttermcorr_2D(
            reformdata[0, :],
            flipfac * reformdata[1, :],
            sampletime,
            windowtime,
            samplestep=int(stepsize // sampletime),
            weighting=args.corrweighting,
            windowfunc=args.windowfunc,
            detrendorder=0,
            display=False,
        )
        tide_io.writenpvecs(corrpertime, args.outfilename + "_pearson.txt")
        tide_io.writenpvecs(ppertime, args.outfilename + "_pvalue.txt")
        tide_io.writenpvecs(Rvals, args.outfilename + "_Rvalue.txt")
        tide_io.writenpvecs(delayvals, args.outfilename + "_delay.txt")
        tide_io.writenpvecs(valid, args.outfilename + "_mask.txt")
        filtereddata1 = tide_math.corrnormalize(
            theprefilter.apply(args.samplerate, trimmeddata[0, :]),
            windowfunc="None",
            detrendorder=args.detrendorder,
        )
        filtereddata2 = tide_math.corrnormalize(
            theprefilter.apply(args.samplerate, trimmeddata[1, :]),
            windowfunc="None",
            detrendorder=args.detrendorder,
        )

        if args.display:
            # timeaxis = np.r_[0.0:len(filtereddata1)] * sampletime
            fig, ax1 = plt.subplots()
            ax1.plot(times, corrpertime, "k")
            ax1.set_ylabel("Pearson R", color="k")
            ax2 = ax1.twinx()
            ax2.plot(times, ppertime, "r")
            ax2.set_ylabel("p value", color="r")
            fig, ax3 = plt.subplots()
            ax3.plot(times, Rvals, "k")
            ax3.set_ylabel("Xcorr max R", color="k")
            ax4 = ax3.twinx()
            ax4.plot(times, delayvals, "r")
            ax4.set_ylabel("Delay (s)", color="r")
            # ax2.set_yscale('log')
            plt.show()


if __name__ == "__main__":
    main()
